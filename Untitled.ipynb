{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date      imf1      imf2      imf3      imf4      imf5      imf6  \\\n",
      "0 1986-01-02 -0.327365  0.173387 -0.625464  2.416827  3.584146  3.518510   \n",
      "1 1986-01-03 -0.084845  0.325226 -0.645562  2.509058  3.597476  3.496085   \n",
      "2 1986-01-06  0.319082  0.365278 -0.535123  2.531153  3.594384  3.470034   \n",
      "3 1986-01-07 -0.310130  0.182301 -0.290603  2.485214  3.574936  3.440430   \n",
      "4 1986-01-08 -0.098686 -0.124846  0.021339  2.374503  3.539778  3.407369   \n",
      "\n",
      "       imf7      imf8      imf9      imf10  \n",
      "0 -1.362703  1.056863 -1.262136  18.387935  \n",
      "1 -1.382060  1.060703 -1.263064  18.386984  \n",
      "2 -1.401384  1.064524 -1.263981  18.386034  \n",
      "3 -1.420674  1.068328 -1.264886  18.385085  \n",
      "4 -1.439928  1.072113 -1.265779  18.384137  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\songz\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22f881cc438>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   # Import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib.dates import DateFormatter, WeekdayLocator,\\\n",
    "    DayLocator, MONDAY\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Permute\n",
    "from keras.layers import merge, Input, concatenate, average, add\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "data=pd.read_excel(r'F:\\研究生阶段\\毕业设计\\处理数据\\EMD分解.xls')\n",
    "'''\n",
    "先对imf1进行预测\n",
    "'''\n",
    "print(data.head())\n",
    "\n",
    "data1=data[['Date','imf1']]\n",
    "data2=data[['Date','imf1']].set_index('Date')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(data1['Date'],data1['imf1'],linestyle='-',linewidth=1,label='imf1')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53957697]\n",
      " [0.47588469]\n",
      " [0.53194665]\n",
      " ...\n",
      " [0.51492709]\n",
      " [0.5121738 ]\n",
      " [0.50926858]]\n",
      "[[0.48077604]\n",
      " [0.49927388]\n",
      " [0.5300828 ]\n",
      " ...\n",
      " [0.46443916]\n",
      " [0.54566119]\n",
      " [0.46843904]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "最大最小归一化数据\n",
    "'''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_train=data2[:-2000].values\n",
    "x_test=data2[-2000:].values\n",
    "min_max_scaler=MinMaxScaler()\n",
    "x_train_scale=min_max_scaler.fit_transform(x_train)\n",
    "x_test_scale=min_max_scaler.transform(data2[-2000:])\n",
    "print(x_test_scale)\n",
    "print(x_train_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建时间窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index, TIME_STEPS):\n",
    "    # y_col_index 是作为输出列的列号\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS #number of time-series samples \n",
    "    dim_1 = mat.shape[1]# number of features\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    \n",
    "    for i in range(0,dim_0):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]#‘y_col_index’ is the index of your output column\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修减数据集，使样本数是batch_size的倍数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size \n",
    "    #here the input 'mat' should be the overall input time series data which has already been devided into windows \n",
    "    #这里的mat应该是已经被划分时间窗口的总体的时间序列数据\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of time-series i/o (6585, 1, 1) (6585,)\n",
      "length of time-series i/o (1999, 1, 1) (1999,)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=20\n",
    "TIME_STEPS=1\n",
    "epoch=50\n",
    "# lr=0.01\n",
    "x_t, y_t = build_timeseries(x_train_scale,0, TIME_STEPS)#data for train\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "x_temp, y_temp = build_timeseries(x_test_scale,0,TIME_STEPS)# x_temp contains the data for validation as well as for test\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp,BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "Date_train=data2[:-2000].index\n",
    "Date_test=data2[-2000:].index\n",
    "Date_val, Date_test = np.split(trim_dataset(Date_test, BATCH_SIZE),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "lr=0.01\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True,kernel_initializer='random_uniform'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(20,activation='relu'))\n",
    "lstm_model.add(Dense(1))\n",
    "# optimizer = optimizers.RMSprop(lr=lr)\n",
    "lstm_model.compile(loss='mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6580 samples, validate on 980 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.0124 - val_loss: 0.0044\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.0031 - val_loss: 0.0026\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 4/50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "OUTPUT_PATH=\"F:\\\\研究生阶段\\\\机器学习\\\\深度学习\\\\LSTM实现\"\n",
    "csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'your_log_name' + '.log'), append=True)\n",
    "\n",
    "history = lstm_model.fit(x_t, y_t, epochs=epoch, verbose=2, batch_size=BATCH_SIZE,\n",
    "                    shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),trim_dataset(y_val, BATCH_SIZE)), callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "pred = lstm_model.predict(trim_dataset(x_test_t,BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "Date_test=pd.to_datetime(Date_test.date)[-len(pred):]\n",
    "predicted = pred\n",
    "original = trim_dataset(y_test_t,BATCH_SIZE)\n",
    "invert_pred=min_max_scaler.inverse_transform(pred)\n",
    "original=min_max_scaler.inverse_transform(original.reshape(-1,1))\n",
    "\n",
    "error = mean_squared_error(original, invert_pred)\n",
    "print(\"Error is\", error)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.title('Actual and predicted')\n",
    "plt.plot(Date_test,original, label = 'Original data')\n",
    "plt.plot(Date_test,invert_pred, label = 'Predicted data')\n",
    "plt.legend(['original', 'prediction'],loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([-1,-2,-3])\n",
    "b=a.reshape(-1,1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
